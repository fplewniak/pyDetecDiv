#    This file was created by
#    MATLAB Deep Learning Toolbox Converter for TensorFlow Models.
#    03-Nov-2023 11:29:04

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def create_model():
    input_1_unnormalized = keras.Input(shape=(299,299,3), name="input_1_unnormalized")
    input_1 = keras.layers.Normalization(axis=(1,2,3), name="input_1_")(input_1_unnormalized)
    conv2d_1 = layers.Conv2D(32, (3,3), strides=(2,2), name="conv2d_1_")(input_1)
    batch_normalization_1 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_1_")(conv2d_1)
    activation_1_relu = layers.ReLU()(batch_normalization_1)
    conv2d_2 = layers.Conv2D(32, (3,3), name="conv2d_2_")(activation_1_relu)
    batch_normalization_2 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_2_")(conv2d_2)
    activation_2_relu = layers.ReLU()(batch_normalization_2)
    conv2d_3 = layers.Conv2D(64, (3,3), padding="same", name="conv2d_3_")(activation_2_relu)
    batch_normalization_3 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_3_")(conv2d_3)
    activation_3_relu = layers.ReLU()(batch_normalization_3)
    max_pooling2d_1 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(activation_3_relu)
    conv2d_4 = layers.Conv2D(80, (1,1), name="conv2d_4_")(max_pooling2d_1)
    batch_normalization_4 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_4_")(conv2d_4)
    activation_4_relu = layers.ReLU()(batch_normalization_4)
    conv2d_5 = layers.Conv2D(192, (3,3), name="conv2d_5_")(activation_4_relu)
    batch_normalization_5 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_5_")(conv2d_5)
    activation_5_relu = layers.ReLU()(batch_normalization_5)
    max_pooling2d_2 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(activation_5_relu)
    conv2d_9 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_9_")(max_pooling2d_2)
    batch_normalization_9 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_9_")(conv2d_9)
    activation_9_relu = layers.ReLU()(batch_normalization_9)
    conv2d_7 = layers.Conv2D(48, (1,1), padding="same", name="conv2d_7_")(max_pooling2d_2)
    conv2d_10 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_10_")(activation_9_relu)
    batch_normalization_7 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_7_")(conv2d_7)
    batch_normalization_10 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_10_")(conv2d_10)
    activation_7_relu = layers.ReLU()(batch_normalization_7)
    activation_10_relu = layers.ReLU()(batch_normalization_10)
    average_pooling2d_1_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(max_pooling2d_2)
    average_pooling2d_1 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_1_prepadded)
    conv2d_6 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_6_")(max_pooling2d_2)
    conv2d_8 = layers.Conv2D(64, (5,5), padding="same", name="conv2d_8_")(activation_7_relu)
    conv2d_11 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_11_")(activation_10_relu)
    conv2d_12 = layers.Conv2D(32, (1,1), padding="same", name="conv2d_12_")(average_pooling2d_1)
    batch_normalization_6 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_6_")(conv2d_6)
    batch_normalization_8 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_8_")(conv2d_8)
    batch_normalization_11 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_11_")(conv2d_11)
    batch_normalization_12 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_12_")(conv2d_12)
    activation_6_relu = layers.ReLU()(batch_normalization_6)
    activation_8_relu = layers.ReLU()(batch_normalization_8)
    activation_11_relu = layers.ReLU()(batch_normalization_11)
    activation_12_relu = layers.ReLU()(batch_normalization_12)
    mixed0 = layers.Concatenate(axis=-1)([activation_6_relu, activation_8_relu, activation_11_relu, activation_12_relu])
    conv2d_16 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_16_")(mixed0)
    batch_normalization_16 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_16_")(conv2d_16)
    activation_16_relu = layers.ReLU()(batch_normalization_16)
    conv2d_14 = layers.Conv2D(48, (1,1), padding="same", name="conv2d_14_")(mixed0)
    conv2d_17 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_17_")(activation_16_relu)
    batch_normalization_14 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_14_")(conv2d_14)
    batch_normalization_17 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_17_")(conv2d_17)
    activation_14_relu = layers.ReLU()(batch_normalization_14)
    activation_17_relu = layers.ReLU()(batch_normalization_17)
    average_pooling2d_2_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed0)
    average_pooling2d_2 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_2_prepadded)
    conv2d_13 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_13_")(mixed0)
    conv2d_15 = layers.Conv2D(64, (5,5), padding="same", name="conv2d_15_")(activation_14_relu)
    conv2d_18 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_18_")(activation_17_relu)
    conv2d_19 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_19_")(average_pooling2d_2)
    batch_normalization_13 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_13_")(conv2d_13)
    batch_normalization_15 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_15_")(conv2d_15)
    batch_normalization_18 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_18_")(conv2d_18)
    batch_normalization_19 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_19_")(conv2d_19)
    activation_13_relu = layers.ReLU()(batch_normalization_13)
    activation_15_relu = layers.ReLU()(batch_normalization_15)
    activation_18_relu = layers.ReLU()(batch_normalization_18)
    activation_19_relu = layers.ReLU()(batch_normalization_19)
    mixed1 = layers.Concatenate(axis=-1)([activation_13_relu, activation_15_relu, activation_18_relu, activation_19_relu])
    conv2d_23 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_23_")(mixed1)
    batch_normalization_23 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_23_")(conv2d_23)
    activation_23_relu = layers.ReLU()(batch_normalization_23)
    conv2d_21 = layers.Conv2D(48, (1,1), padding="same", name="conv2d_21_")(mixed1)
    conv2d_24 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_24_")(activation_23_relu)
    batch_normalization_21 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_21_")(conv2d_21)
    batch_normalization_24 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_24_")(conv2d_24)
    activation_21_relu = layers.ReLU()(batch_normalization_21)
    activation_24_relu = layers.ReLU()(batch_normalization_24)
    average_pooling2d_3_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed1)
    average_pooling2d_3 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_3_prepadded)
    conv2d_20 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_20_")(mixed1)
    conv2d_22 = layers.Conv2D(64, (5,5), padding="same", name="conv2d_22_")(activation_21_relu)
    conv2d_25 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_25_")(activation_24_relu)
    conv2d_26 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_26_")(average_pooling2d_3)
    batch_normalization_20 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_20_")(conv2d_20)
    batch_normalization_22 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_22_")(conv2d_22)
    batch_normalization_25 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_25_")(conv2d_25)
    batch_normalization_26 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_26_")(conv2d_26)
    activation_20_relu = layers.ReLU()(batch_normalization_20)
    activation_22_relu = layers.ReLU()(batch_normalization_22)
    activation_25_relu = layers.ReLU()(batch_normalization_25)
    activation_26_relu = layers.ReLU()(batch_normalization_26)
    mixed2 = layers.Concatenate(axis=-1)([activation_20_relu, activation_22_relu, activation_25_relu, activation_26_relu])
    conv2d_28 = layers.Conv2D(64, (1,1), padding="same", name="conv2d_28_")(mixed2)
    batch_normalization_28 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_28_")(conv2d_28)
    activation_28_relu = layers.ReLU()(batch_normalization_28)
    conv2d_29 = layers.Conv2D(96, (3,3), padding="same", name="conv2d_29_")(activation_28_relu)
    batch_normalization_29 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_29_")(conv2d_29)
    activation_29_relu = layers.ReLU()(batch_normalization_29)
    conv2d_27 = layers.Conv2D(384, (3,3), strides=(2,2), name="conv2d_27_")(mixed2)
    conv2d_30 = layers.Conv2D(96, (3,3), strides=(2,2), name="conv2d_30_")(activation_29_relu)
    batch_normalization_27 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_27_")(conv2d_27)
    batch_normalization_30 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_30_")(conv2d_30)
    activation_27_relu = layers.ReLU()(batch_normalization_27)
    activation_30_relu = layers.ReLU()(batch_normalization_30)
    max_pooling2d_3 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(mixed2)
    mixed3 = layers.Concatenate(axis=-1)([activation_27_relu, activation_30_relu, max_pooling2d_3])
    conv2d_35 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_35_")(mixed3)
    batch_normalization_35 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_35_")(conv2d_35)
    activation_35_relu = layers.ReLU()(batch_normalization_35)
    conv2d_36 = layers.Conv2D(128, (7,1), padding="same", name="conv2d_36_")(activation_35_relu)
    batch_normalization_36 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_36_")(conv2d_36)
    activation_36_relu = layers.ReLU()(batch_normalization_36)
    conv2d_32 = layers.Conv2D(128, (1,1), padding="same", name="conv2d_32_")(mixed3)
    conv2d_37 = layers.Conv2D(128, (1,7), padding="same", name="conv2d_37_")(activation_36_relu)
    batch_normalization_32 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_32_")(conv2d_32)
    batch_normalization_37 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_37_")(conv2d_37)
    activation_32_relu = layers.ReLU()(batch_normalization_32)
    activation_37_relu = layers.ReLU()(batch_normalization_37)
    conv2d_33 = layers.Conv2D(128, (1,7), padding="same", name="conv2d_33_")(activation_32_relu)
    conv2d_38 = layers.Conv2D(128, (7,1), padding="same", name="conv2d_38_")(activation_37_relu)
    batch_normalization_33 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_33_")(conv2d_33)
    batch_normalization_38 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_38_")(conv2d_38)
    activation_33_relu = layers.ReLU()(batch_normalization_33)
    activation_38_relu = layers.ReLU()(batch_normalization_38)
    average_pooling2d_4_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed3)
    average_pooling2d_4 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_4_prepadded)
    conv2d_31 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_31_")(mixed3)
    conv2d_34 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_34_")(activation_33_relu)
    conv2d_39 = layers.Conv2D(192, (1,7), padding="same", name="conv2d_39_")(activation_38_relu)
    conv2d_40 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_40_")(average_pooling2d_4)
    batch_normalization_31 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_31_")(conv2d_31)
    batch_normalization_34 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_34_")(conv2d_34)
    batch_normalization_39 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_39_")(conv2d_39)
    batch_normalization_40 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_40_")(conv2d_40)
    activation_31_relu = layers.ReLU()(batch_normalization_31)
    activation_34_relu = layers.ReLU()(batch_normalization_34)
    activation_39_relu = layers.ReLU()(batch_normalization_39)
    activation_40_relu = layers.ReLU()(batch_normalization_40)
    mixed4 = layers.Concatenate(axis=-1)([activation_31_relu, activation_34_relu, activation_39_relu, activation_40_relu])
    conv2d_45 = layers.Conv2D(160, (1,1), padding="same", name="conv2d_45_")(mixed4)
    batch_normalization_45 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_45_")(conv2d_45)
    activation_45_relu = layers.ReLU()(batch_normalization_45)
    conv2d_46 = layers.Conv2D(160, (7,1), padding="same", name="conv2d_46_")(activation_45_relu)
    batch_normalization_46 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_46_")(conv2d_46)
    activation_46_relu = layers.ReLU()(batch_normalization_46)
    conv2d_42 = layers.Conv2D(160, (1,1), padding="same", name="conv2d_42_")(mixed4)
    conv2d_47 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_47_")(activation_46_relu)
    batch_normalization_42 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_42_")(conv2d_42)
    batch_normalization_47 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_47_")(conv2d_47)
    activation_42_relu = layers.ReLU()(batch_normalization_42)
    activation_47_relu = layers.ReLU()(batch_normalization_47)
    conv2d_43 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_43_")(activation_42_relu)
    conv2d_48 = layers.Conv2D(160, (7,1), padding="same", name="conv2d_48_")(activation_47_relu)
    batch_normalization_43 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_43_")(conv2d_43)
    batch_normalization_48 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_48_")(conv2d_48)
    activation_43_relu = layers.ReLU()(batch_normalization_43)
    activation_48_relu = layers.ReLU()(batch_normalization_48)
    average_pooling2d_5_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed4)
    average_pooling2d_5 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_5_prepadded)
    conv2d_41 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_41_")(mixed4)
    conv2d_44 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_44_")(activation_43_relu)
    conv2d_49 = layers.Conv2D(192, (1,7), padding="same", name="conv2d_49_")(activation_48_relu)
    conv2d_50 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_50_")(average_pooling2d_5)
    batch_normalization_41 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_41_")(conv2d_41)
    batch_normalization_44 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_44_")(conv2d_44)
    batch_normalization_49 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_49_")(conv2d_49)
    batch_normalization_50 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_50_")(conv2d_50)
    activation_41_relu = layers.ReLU()(batch_normalization_41)
    activation_44_relu = layers.ReLU()(batch_normalization_44)
    activation_49_relu = layers.ReLU()(batch_normalization_49)
    activation_50_relu = layers.ReLU()(batch_normalization_50)
    mixed5 = layers.Concatenate(axis=-1)([activation_41_relu, activation_44_relu, activation_49_relu, activation_50_relu])
    conv2d_55 = layers.Conv2D(160, (1,1), padding="same", name="conv2d_55_")(mixed5)
    batch_normalization_55 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_55_")(conv2d_55)
    activation_55_relu = layers.ReLU()(batch_normalization_55)
    conv2d_56 = layers.Conv2D(160, (7,1), padding="same", name="conv2d_56_")(activation_55_relu)
    batch_normalization_56 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_56_")(conv2d_56)
    activation_56_relu = layers.ReLU()(batch_normalization_56)
    conv2d_52 = layers.Conv2D(160, (1,1), padding="same", name="conv2d_52_")(mixed5)
    conv2d_57 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_57_")(activation_56_relu)
    batch_normalization_52 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_52_")(conv2d_52)
    batch_normalization_57 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_57_")(conv2d_57)
    activation_52_relu = layers.ReLU()(batch_normalization_52)
    activation_57_relu = layers.ReLU()(batch_normalization_57)
    conv2d_53 = layers.Conv2D(160, (1,7), padding="same", name="conv2d_53_")(activation_52_relu)
    conv2d_58 = layers.Conv2D(160, (7,1), padding="same", name="conv2d_58_")(activation_57_relu)
    batch_normalization_53 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_53_")(conv2d_53)
    batch_normalization_58 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_58_")(conv2d_58)
    activation_53_relu = layers.ReLU()(batch_normalization_53)
    activation_58_relu = layers.ReLU()(batch_normalization_58)
    average_pooling2d_6_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed5)
    average_pooling2d_6 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_6_prepadded)
    conv2d_51 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_51_")(mixed5)
    conv2d_54 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_54_")(activation_53_relu)
    conv2d_59 = layers.Conv2D(192, (1,7), padding="same", name="conv2d_59_")(activation_58_relu)
    conv2d_60 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_60_")(average_pooling2d_6)
    batch_normalization_51 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_51_")(conv2d_51)
    batch_normalization_54 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_54_")(conv2d_54)
    batch_normalization_59 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_59_")(conv2d_59)
    batch_normalization_60 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_60_")(conv2d_60)
    activation_51_relu = layers.ReLU()(batch_normalization_51)
    activation_54_relu = layers.ReLU()(batch_normalization_54)
    activation_59_relu = layers.ReLU()(batch_normalization_59)
    activation_60_relu = layers.ReLU()(batch_normalization_60)
    mixed6 = layers.Concatenate(axis=-1)([activation_51_relu, activation_54_relu, activation_59_relu, activation_60_relu])
    conv2d_65 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_65_")(mixed6)
    batch_normalization_65 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_65_")(conv2d_65)
    activation_65_relu = layers.ReLU()(batch_normalization_65)
    conv2d_66 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_66_")(activation_65_relu)
    batch_normalization_66 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_66_")(conv2d_66)
    activation_66_relu = layers.ReLU()(batch_normalization_66)
    conv2d_62 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_62_")(mixed6)
    conv2d_67 = layers.Conv2D(192, (1,7), padding="same", name="conv2d_67_")(activation_66_relu)
    batch_normalization_62 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_62_")(conv2d_62)
    batch_normalization_67 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_67_")(conv2d_67)
    activation_62_relu = layers.ReLU()(batch_normalization_62)
    activation_67_relu = layers.ReLU()(batch_normalization_67)
    conv2d_63 = layers.Conv2D(192, (1,7), padding="same", name="conv2d_63_")(activation_62_relu)
    conv2d_68 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_68_")(activation_67_relu)
    batch_normalization_63 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_63_")(conv2d_63)
    batch_normalization_68 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_68_")(conv2d_68)
    activation_63_relu = layers.ReLU()(batch_normalization_63)
    activation_68_relu = layers.ReLU()(batch_normalization_68)
    average_pooling2d_7_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed6)
    average_pooling2d_7 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_7_prepadded)
    conv2d_61 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_61_")(mixed6)
    conv2d_64 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_64_")(activation_63_relu)
    conv2d_69 = layers.Conv2D(192, (1,7), padding="same", name="conv2d_69_")(activation_68_relu)
    conv2d_70 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_70_")(average_pooling2d_7)
    batch_normalization_61 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_61_")(conv2d_61)
    batch_normalization_64 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_64_")(conv2d_64)
    batch_normalization_69 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_69_")(conv2d_69)
    batch_normalization_70 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_70_")(conv2d_70)
    activation_61_relu = layers.ReLU()(batch_normalization_61)
    activation_64_relu = layers.ReLU()(batch_normalization_64)
    activation_69_relu = layers.ReLU()(batch_normalization_69)
    activation_70_relu = layers.ReLU()(batch_normalization_70)
    mixed7 = layers.Concatenate(axis=-1)([activation_61_relu, activation_64_relu, activation_69_relu, activation_70_relu])
    conv2d_73 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_73_")(mixed7)
    batch_normalization_73 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_73_")(conv2d_73)
    activation_73_relu = layers.ReLU()(batch_normalization_73)
    conv2d_74 = layers.Conv2D(192, (1,7), padding="same", name="conv2d_74_")(activation_73_relu)
    batch_normalization_74 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_74_")(conv2d_74)
    activation_74_relu = layers.ReLU()(batch_normalization_74)
    conv2d_71 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_71_")(mixed7)
    conv2d_75 = layers.Conv2D(192, (7,1), padding="same", name="conv2d_75_")(activation_74_relu)
    batch_normalization_71 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_71_")(conv2d_71)
    batch_normalization_75 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_75_")(conv2d_75)
    activation_71_relu = layers.ReLU()(batch_normalization_71)
    activation_75_relu = layers.ReLU()(batch_normalization_75)
    conv2d_72 = layers.Conv2D(320, (3,3), strides=(2,2), name="conv2d_72_")(activation_71_relu)
    conv2d_76 = layers.Conv2D(192, (3,3), strides=(2,2), name="conv2d_76_")(activation_75_relu)
    batch_normalization_72 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_72_")(conv2d_72)
    batch_normalization_76 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_76_")(conv2d_76)
    activation_72_relu = layers.ReLU()(batch_normalization_72)
    activation_76_relu = layers.ReLU()(batch_normalization_76)
    max_pooling2d_4 = layers.MaxPool2D(pool_size=(3,3), strides=(2,2))(mixed7)
    mixed8 = layers.Concatenate(axis=-1)([activation_72_relu, activation_76_relu, max_pooling2d_4])
    conv2d_81 = layers.Conv2D(448, (1,1), padding="same", name="conv2d_81_")(mixed8)
    batch_normalization_81 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_81_")(conv2d_81)
    activation_81_relu = layers.ReLU()(batch_normalization_81)
    conv2d_78 = layers.Conv2D(384, (1,1), padding="same", name="conv2d_78_")(mixed8)
    conv2d_82 = layers.Conv2D(384, (3,3), padding="same", name="conv2d_82_")(activation_81_relu)
    batch_normalization_78 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_78_")(conv2d_78)
    batch_normalization_82 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_82_")(conv2d_82)
    activation_78_relu = layers.ReLU()(batch_normalization_78)
    activation_82_relu = layers.ReLU()(batch_normalization_82)
    conv2d_79 = layers.Conv2D(384, (1,3), padding="same", name="conv2d_79_")(activation_78_relu)
    conv2d_80 = layers.Conv2D(384, (3,1), padding="same", name="conv2d_80_")(activation_78_relu)
    conv2d_83 = layers.Conv2D(384, (1,3), padding="same", name="conv2d_83_")(activation_82_relu)
    conv2d_84 = layers.Conv2D(384, (3,1), padding="same", name="conv2d_84_")(activation_82_relu)
    average_pooling2d_8_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed8)
    average_pooling2d_8 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_8_prepadded)
    conv2d_77 = layers.Conv2D(320, (1,1), padding="same", name="conv2d_77_")(mixed8)
    batch_normalization_79 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_79_")(conv2d_79)
    batch_normalization_80 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_80_")(conv2d_80)
    batch_normalization_83 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_83_")(conv2d_83)
    batch_normalization_84 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_84_")(conv2d_84)
    conv2d_85 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_85_")(average_pooling2d_8)
    batch_normalization_77 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_77_")(conv2d_77)
    activation_79_relu = layers.ReLU()(batch_normalization_79)
    activation_80_relu = layers.ReLU()(batch_normalization_80)
    activation_83_relu = layers.ReLU()(batch_normalization_83)
    activation_84_relu = layers.ReLU()(batch_normalization_84)
    batch_normalization_85 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_85_")(conv2d_85)
    activation_77_relu = layers.ReLU()(batch_normalization_77)
    mixed9_0 = layers.Concatenate(axis=-1)([activation_79_relu, activation_80_relu])
    concatenate_1 = layers.Concatenate(axis=-1)([activation_83_relu, activation_84_relu])
    activation_85_relu = layers.ReLU()(batch_normalization_85)
    mixed9 = layers.Concatenate(axis=-1)([activation_77_relu, mixed9_0, concatenate_1, activation_85_relu])
    conv2d_90 = layers.Conv2D(448, (1,1), padding="same", name="conv2d_90_")(mixed9)
    batch_normalization_90 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_90_")(conv2d_90)
    activation_90_relu = layers.ReLU()(batch_normalization_90)
    conv2d_87 = layers.Conv2D(384, (1,1), padding="same", name="conv2d_87_")(mixed9)
    conv2d_91 = layers.Conv2D(384, (3,3), padding="same", name="conv2d_91_")(activation_90_relu)
    batch_normalization_87 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_87_")(conv2d_87)
    batch_normalization_91 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_91_")(conv2d_91)
    activation_87_relu = layers.ReLU()(batch_normalization_87)
    activation_91_relu = layers.ReLU()(batch_normalization_91)
    conv2d_88 = layers.Conv2D(384, (1,3), padding="same", name="conv2d_88_")(activation_87_relu)
    conv2d_89 = layers.Conv2D(384, (3,1), padding="same", name="conv2d_89_")(activation_87_relu)
    conv2d_92 = layers.Conv2D(384, (1,3), padding="same", name="conv2d_92_")(activation_91_relu)
    conv2d_93 = layers.Conv2D(384, (3,1), padding="same", name="conv2d_93_")(activation_91_relu)
    average_pooling2d_9_prepadded = layers.ZeroPadding2D(padding=((1,1),(1,1)))(mixed9)
    average_pooling2d_9 = layers.AveragePooling2D(pool_size=(3,3), strides=(1,1))(average_pooling2d_9_prepadded)
    conv2d_86 = layers.Conv2D(320, (1,1), padding="same", name="conv2d_86_")(mixed9)
    batch_normalization_88 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_88_")(conv2d_88)
    batch_normalization_89 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_89_")(conv2d_89)
    batch_normalization_92 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_92_")(conv2d_92)
    batch_normalization_93 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_93_")(conv2d_93)
    conv2d_94 = layers.Conv2D(192, (1,1), padding="same", name="conv2d_94_")(average_pooling2d_9)
    batch_normalization_86 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_86_")(conv2d_86)
    activation_88_relu = layers.ReLU()(batch_normalization_88)
    activation_89_relu = layers.ReLU()(batch_normalization_89)
    activation_92_relu = layers.ReLU()(batch_normalization_92)
    activation_93_relu = layers.ReLU()(batch_normalization_93)
    batch_normalization_94 = layers.BatchNormalization(epsilon=0.001000, name="batch_normalization_94_")(conv2d_94)
    activation_86_relu = layers.ReLU()(batch_normalization_86)
    mixed9_1 = layers.Concatenate(axis=-1)([activation_88_relu, activation_89_relu])
    concatenate_2 = layers.Concatenate(axis=-1)([activation_92_relu, activation_93_relu])
    activation_94_relu = layers.ReLU()(batch_normalization_94)
    mixed10 = layers.Concatenate(axis=-1)([activation_86_relu, mixed9_1, concatenate_2, activation_94_relu])
    avg_pool = layers.GlobalAveragePooling2D(keepdims=True)(mixed10)
    new_fc = layers.Reshape((1, 1, -1), name="new_fc_preFlatten1")(avg_pool)
    new_fc = layers.Dense(6, name="new_fc_")(new_fc)
    predictions_softmax = layers.Softmax()(new_fc)
    new_classoutput = predictions_softmax

    model = keras.Model(inputs=[input_1_unnormalized], outputs=[input_1, new_classoutput])
    return model
